{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBUST04 Ranking Pipeline\n",
    "\n",
    "**Commands:** `train` (50 queries + evaluate) | `test` (199 queries for submission)\n",
    "\n",
    "**Runs:** 1. BM25+RM3+Q2D → 2. Neural Reranking → 3. RRF Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!rm -rf text-retrieval-and-search-engines\n",
    "!git clone https://github.com/er1009/text-retrieval-and-search-engines.git\n",
    "%cd text-retrieval-and-search-engines/final-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq && apt-get install -qq openjdk-21-jdk-headless > /dev/null 2>&1\n",
    "%pip install -q pyserini faiss-cpu torch transformers sentence-transformers \\\n",
    "    pytrec_eval langchain-text-splitters tqdm accelerate 2>/dev/null\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "s = LuceneSearcher.from_prebuilt_index('robust04'); print(f\"✓ Index ({s.num_docs:,} docs)\"); s.close()\n",
    "SentenceTransformer('BAAI/bge-base-en-v1.5', device='cpu'); print(\"✓ Bi-Encoder\")\n",
    "CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cpu'); print(\"✓ Cross-Encoder L6\")\n",
    "CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device='cpu'); print(\"✓ Cross-Encoder L12\")\n",
    "AutoTokenizer.from_pretrained('castorini/monot5-base-msmarco')\n",
    "T5ForConditionalGeneration.from_pretrained('castorini/monot5-base-msmarco', torch_dtype=torch.bfloat16)\n",
    "print(\"✓ MonoT5\")\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n✓ All models cached!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train (Run + Evaluate)\n",
    "\n",
    "Runs all 3 methods on **50 training queries** and evaluates.\n",
    "\n",
    "**Note:** `--bi-encoder-top-k` auto-scales with `--rerank-depth` if not specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.main train \\\n",
    "    --output-dir results \\\n",
    "    --bm25-k 1000 \\\n",
    "    --rerank-depth 1000 \\\n",
    "    --chunk-size 256 \\\n",
    "    --chunk-overlap 64 \\\n",
    "    --bi-batch-size 2048 \\\n",
    "    --ce-batch-size 2048 \\\n",
    "    --monot5-batch-size 512 \\\n",
    "    --ce-weight 0.5 \\\n",
    "    --neural-weight 0.8 \\\n",
    "    --rrf-k 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test (Submission)\n",
    "\n",
    "Runs all 3 methods on **199 test queries** for competition submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.main test \\\n",
    "    --output-dir submission \\\n",
    "    --bm25-k 1000 \\\n",
    "    --rerank-depth 1000 \\\n",
    "    --chunk-size 256 \\\n",
    "    --chunk-overlap 64 \\\n",
    "    --bi-batch-size 2048 \\\n",
    "    --ce-batch-size 2048 \\\n",
    "    --monot5-batch-size 512 \\\n",
    "    --ce-weight 0.5 \\\n",
    "    --neural-weight 0.8 \\\n",
    "    --rrf-k 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Package & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"Query counts (should be 199 each):\"\n",
    "!for f in submission/run_*.res; do echo -n \"$f: \"; cut -d' ' -f1 $f | sort -u | wc -l; done\n",
    "!cd submission && zip -r ../Final_Project_Part_A.zip run_1.res run_2.res run_3.res\n",
    "!ls -la Final_Project_Part_A.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('Final_Project_Part_A.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parameters Reference\n",
    "\n",
    "| Parameter | Default | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `--config` | None | Tuned config JSON |\n",
    "| `--output-dir` | results | Output directory |\n",
    "| `--bm25-k` | 1000 | BM25 retrieval depth |\n",
    "| `--rerank-depth` | 1000 | Docs to rerank |\n",
    "| `--chunk-size` | 256 | Chunk size (chars) |\n",
    "| `--chunk-overlap` | 64 | Chunk overlap (chars) |\n",
    "| `--bi-encoder-top-k` | **auto** | Auto-scales: min(depth×15, max(3000, depth×8)) |\n",
    "| `--bi-batch-size` | 2048 | Bi-encoder batch |\n",
    "| `--ce-batch-size` | 2048 | Cross-encoder batch |\n",
    "| `--monot5-batch-size` | 512 | MonoT5 batch |\n",
    "| `--ce-weight` | 0.5 | CE weight in ensemble |\n",
    "| `--neural-weight` | 0.8 | Neural/BM25 weight |\n",
    "| `--rrf-k` | 60 | RRF k parameter |\n",
    "| `--no-gpu` | flag | Disable GPU |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
