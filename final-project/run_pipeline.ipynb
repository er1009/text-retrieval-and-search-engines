{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROBUST04 Ranking Competition - THREE-STAGE NEURAL Pipeline\n",
        "\n",
        "This notebook runs the complete pipeline for the ROBUST04 ranking competition.\n",
        "**Optimized for A100 80GB GPU** - uses **THREE-STAGE neural reranking**!\n",
        "\n",
        "**Neural Pipeline (Run 2):**\n",
        "```\n",
        "BM25 (1000) ‚Üí Bi-Encoder (500) ‚Üí Cross-Encoder + MonoT5-3B ‚Üí Final\n",
        "   Fast           Fast              Precise ensemble\n",
        "```\n",
        "\n",
        "**Three Runs:**\n",
        "1. **Run 1**: BM25 + RM3 + Claude Query2Doc (lexical + PRF + semantic expansion)\n",
        "2. **Run 2**: Three-Stage Neural Reranking (Bi-Encoder ‚Üí BGE-v2-m3 ‚Üí MonoT5-3B)\n",
        "3. **Run 3**: Optimal Multi-Signal RRF Fusion\n",
        "\n",
        "**Expected Performance:**\n",
        "- Run 1 (Lexical): MAP ~0.33-0.36\n",
        "- Run 2 (Neural): MAP ~0.44-0.50 (three-stage pipeline!)\n",
        "- Run 3 (Fusion): MAP ~0.47-0.52\n",
        "\n",
        "**Resource Usage (A100 80GB):**\n",
        "- GPU Memory: ~17GB (BGE-large ~1.3GB + BGE-reranker ~2.3GB + MonoT5-3B ~12GB)\n",
        "- System RAM: ~10GB\n",
        "- Disk: ~12GB (index + models)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository (replace with your repo URL)\n",
        "!git clone https://github.com/YOUR_USERNAME/text_retrieval.git\n",
        "%cd text_retrieval/final-project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Java (required by pyserini/Lucene)\n",
        "!apt-get update -qq 2>/dev/null\n",
        "!apt-get install -qq openjdk-21-jdk-headless > /dev/null 2>&1\n",
        "\n",
        "# Verify Java installation\n",
        "!java -version\n",
        "\n",
        "# Install Python dependencies\n",
        "# Note: Dependency conflicts with pre-installed Colab packages are harmless\n",
        "%pip install -q pyserini faiss-cpu torch transformers sentence-transformers \\\n",
        "    pytrec_eval langchain langchain-text-splitters tqdm scikit-learn numpy accelerate 2>/dev/null\n",
        "\n",
        "print(\"\\n‚úì All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Pyserini's prebuilt ROBUST04 index\n",
        "print(\"=\" * 60)\n",
        "print(\"DOWNLOADING REQUIRED RESOURCES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n[1/4] Downloading ROBUST04 index...\")\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "searcher = LuceneSearcher.from_prebuilt_index('robust04')\n",
        "print(f\"  ‚úì Index loaded: {searcher.num_docs:,} documents\")\n",
        "searcher.close()\n",
        "\n",
        "# Pre-download neural models to cache them\n",
        "import torch\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Download BGE-base Bi-Encoder (FAST!)\n",
        "print(\"\\n[2/5] Downloading BGE-base Bi-Encoder (BAAI/bge-base-en-v1.5)...\")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "bi_encoder = SentenceTransformer('BAAI/bge-base-en-v1.5', device='cpu')\n",
        "params = sum(p.numel() for p in bi_encoder.parameters()) / 1e6\n",
        "print(f\"  ‚úì BGE-base Bi-Encoder downloaded ({params:.0f}M params)\")\n",
        "del bi_encoder\n",
        "\n",
        "# Download MiniLM-L6 Cross-Encoder (FAST 6-layer model!)\n",
        "print(\"\\n[3/5] Downloading MiniLM-L6 Cross-Encoder...\")\n",
        "ce_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cpu')\n",
        "params = sum(p.numel() for p in ce_model.model.parameters()) / 1e6\n",
        "print(f\"  ‚úì MiniLM-L6 Cross-Encoder downloaded ({params:.0f}M params)\")\n",
        "del ce_model\n",
        "\n",
        "# Download MonoT5-base (faster than 3B!)\n",
        "print(\"\\n[4/5] Downloading MonoT5-base (castorini/monot5-base-msmarco)...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('castorini/monot5-base-msmarco')\n",
        "model = T5ForConditionalGeneration.from_pretrained(\n",
        "    'castorini/monot5-base-msmarco',\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "params_m = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "print(f\"  ‚úì MonoT5-base downloaded ({params_m:.0f}M params)\")\n",
        "del model, tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check GPU memory\n",
        "print(\"\\n[5/5] Checking GPU resources...\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"  ‚úì GPU: {gpu_name} ({gpu_mem:.0f}GB)\")\n",
        "else:\n",
        "    print(\"  ‚ö† No GPU detected - neural reranking will be slow!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úì ALL RESOURCES READY!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation: Test All Components\n",
        "\n",
        "Run this cell to verify all libraries are working correctly before running the full pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate all components work correctly\n",
        "print(\"=\" * 50)\n",
        "print(\"VALIDATION: Testing all components\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test 0: Data files exist and are correct\n",
        "print(\"\\n0. Checking data files...\")\n",
        "import os\n",
        "assert os.path.exists('Files-20260103/queriesROBUST.txt'), \"Queries file missing!\"\n",
        "assert os.path.exists('Files-20260103/qrels_50_Queries'), \"Qrels file missing!\"\n",
        "assert os.path.exists('data/expanded_queries.csv'), \"Expanded queries file missing!\"\n",
        "\n",
        "# Load and verify queries\n",
        "from src.data_loader import load_queries, load_expanded_queries, load_qrels, get_train_qids, get_test_qids\n",
        "queries = load_queries()\n",
        "expanded = load_expanded_queries()\n",
        "qrels = load_qrels()\n",
        "train_qids = get_train_qids()\n",
        "test_qids = get_test_qids()\n",
        "\n",
        "print(f\"   ‚úì Loaded {len(queries)} queries\")\n",
        "print(f\"   ‚úì Loaded {len(expanded)} expanded queries\")\n",
        "print(f\"   ‚úì Loaded qrels for {len(qrels)} queries\")\n",
        "print(f\"   ‚úì Train queries: {len(train_qids)} (301-350)\")\n",
        "print(f\"   ‚úì Test queries: {len(test_qids)} (351-450, 601-700 minus 672)\")\n",
        "assert len(queries) == 249, f\"Expected 249 queries, got {len(queries)}\"\n",
        "assert len(expanded) == 249, f\"Expected 249 expanded queries, got {len(expanded)}\"\n",
        "assert len(qrels) == 50, f\"Expected 50 qrels, got {len(qrels)}\"\n",
        "assert len(train_qids) == 50, f\"Expected 50 train qids, got {len(train_qids)}\"\n",
        "assert len(test_qids) == 199, f\"Expected 199 test qids, got {len(test_qids)}\"\n",
        "print(f\"   ‚úì All counts verified!\")\n",
        "\n",
        "# Test 1: BM25 Search\n",
        "print(\"\\n1. Testing BM25 search...\")\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "searcher = LuceneSearcher.from_prebuilt_index('robust04')\n",
        "searcher.set_bm25(k1=0.9, b=0.4)\n",
        "hits = searcher.search('international organized crime', k=10)\n",
        "print(f\"   ‚úì BM25 returned {len(hits)} results\")\n",
        "print(f\"   Top doc: {hits[0].docid} (score: {hits[0].score:.4f})\")\n",
        "\n",
        "# Test 2: BM25 + RM3 Search\n",
        "print(\"\\n2. Testing BM25 + RM3 search...\")\n",
        "searcher.set_rm3(fb_docs=10, fb_terms=10, original_query_weight=0.5)\n",
        "hits_rm3 = searcher.search('international organized crime', k=10)\n",
        "print(f\"   ‚úì BM25+RM3 returned {len(hits_rm3)} results\")\n",
        "searcher.close()\n",
        "\n",
        "# Test 3: MiniLM-L6 Cross-Encoder (FAST 6-layer model)\n",
        "print(\"\\n3. Testing MiniLM-L6 Cross-Encoder...\")\n",
        "from sentence_transformers import CrossEncoder\n",
        "import torch\n",
        "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cuda')\n",
        "scores = ce.predict([\n",
        "    ('international crime', 'This document discusses transnational criminal organizations.'),\n",
        "    ('international crime', 'The weather today is sunny and warm.')\n",
        "])\n",
        "print(f\"   ‚úì Relevant doc score: {scores[0]:.4f}\")\n",
        "print(f\"   ‚úì Irrelevant doc score: {scores[1]:.4f}\")\n",
        "assert scores[0] > scores[1], \"Relevant doc should score higher!\"\n",
        "print(f\"   ‚úì Sanity check passed!\")\n",
        "del ce\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Test 4: LangChain chunking\n",
        "print(\"\\n4. Testing LangChain chunking...\")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=256, chunk_overlap=64,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \", \", \" \", \"\"]\n",
        ")\n",
        "chunks = splitter.split_text(\"This is a test document. \" * 50)\n",
        "print(f\"   ‚úì Created {len(chunks)} chunks from test text\")\n",
        "\n",
        "# Test 5: pytrec_eval\n",
        "print(\"\\n5. Testing pytrec_eval...\")\n",
        "import pytrec_eval\n",
        "qrels = {'q1': {'d1': 1, 'd2': 0, 'd3': 1}}\n",
        "results = {'q1': {'d1': 0.9, 'd2': 0.5, 'd3': 0.8}}\n",
        "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'ndcg'})\n",
        "metrics = evaluator.evaluate(results)\n",
        "print(f\"   ‚úì MAP: {metrics['q1']['map']:.4f}\")\n",
        "print(f\"   ‚úì NDCG: {metrics['q1']['ndcg']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"‚úì All components validated successfully!\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Parameter Tuning (Optional - Run Once)\n",
        "\n",
        "This tunes BM25 and RM3 parameters using 5-fold cross-validation on the 50 training queries.\n",
        "\n",
        "**Takes ~15-30 minutes.** Results are saved so you only need to run this once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run parameter tuning (optional - skip if using defaults)\n",
        "!python -m src.main tune --output tuning_results/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Generate Run 1 - BM25 + RM3 + Query2Doc\n",
        "\n",
        "Uses Claude's pre-generated query expansions + tuned BM25/RM3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main run1 \\\n",
        "    --config tuning_results/best_config.json \\\n",
        "    --output results/run_1.res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Generate Run 2 - Neural MaxP Reranking\n",
        "\n",
        "Uses Cross-Encoder to rerank passages, then aggregates to document scores using MaxP.\n",
        "\n",
        "**Takes ~30-60 minutes** depending on GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main run2 \\\n",
        "    --config tuning_results/best_config.json \\\n",
        "    --output results/run_2.res \\\n",
        "    --rerank-depth 200 \\\n",
        "    --gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Generate Run 3 - Optimal RRF Fusion\n",
        "\n",
        "Fuses Run 1 and Run 2 using Reciprocal Rank Fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main run3 \\\n",
        "    --run1 results/run_1.res \\\n",
        "    --run2 results/run_2.res \\\n",
        "    --output results/run_3.res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Evaluate on Training Queries\n",
        "\n",
        "Check performance on the 50 training queries (301-350).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on the 50 TRAINING queries (301-350)\n",
        "# The full run files contain ALL queries (train + test) for evaluation purposes\n",
        "# The submission files (*_submission.res) contain only test queries\n",
        "\n",
        "!python -m src.main evaluate \\\n",
        "    results/run_1.res \\\n",
        "    results/run_2.res \\\n",
        "    results/run_3.res\n",
        "\n",
        "print(\"\\nüí° Note: MAP shown above is on TRAINING data only (for tuning).\")\n",
        "print(\"   Competition score will be based on 199 TEST queries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Verify Output Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check file sizes and line counts\n",
        "!echo \"=== Run 1 ===\"\n",
        "!wc -l results/run_1.res\n",
        "!head -5 results/run_1.res\n",
        "!echo \"\"\n",
        "!echo \"=== Run 2 ===\"\n",
        "!wc -l results/run_2.res\n",
        "!head -5 results/run_2.res\n",
        "!echo \"\"\n",
        "!echo \"=== Run 3 ===\"\n",
        "!wc -l results/run_3.res\n",
        "!head -5 results/run_3.res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify query coverage\n",
        "# FULL runs: should have 249 queries (50 train + 199 test)\n",
        "# SUBMISSION runs: should have 199 queries (test only)\n",
        "\n",
        "print(\"=== FULL RUN FILES (for evaluation) ===\")\n",
        "print(\"Expected: 249 queries (50 train + 199 test)\\n\")\n",
        "!echo \"run_1.res:\" && cut -d' ' -f1 results/run_1.res | sort -u | wc -l\n",
        "!echo \"run_2.res:\" && cut -d' ' -f1 results/run_2.res | sort -u | wc -l\n",
        "!echo \"run_3.res:\" && cut -d' ' -f1 results/run_3.res | sort -u | wc -l\n",
        "\n",
        "print(\"\\n=== SUBMISSION FILES (for competition) ===\")\n",
        "print(\"Expected: 199 queries (test only)\\n\")\n",
        "!echo \"run_1_submission.res:\" && cut -d' ' -f1 results/run_1_submission.res | sort -u | wc -l\n",
        "!echo \"run_2_submission.res:\" && cut -d' ' -f1 results/run_2_submission.res | sort -u | wc -l\n",
        "!echo \"run_3_submission.res:\" && cut -d' ' -f1 results/run_3_submission.res | sort -u | wc -l\n",
        "\n",
        "# Verify line counts (should be queries √ó 1000 docs each)\n",
        "print(\"\\n=== LINE COUNTS ===\")\n",
        "print(\"Full runs: 249 √ó 1000 = 249,000 lines expected\")\n",
        "print(\"Submission runs: 199 √ó 1000 = 199,000 lines expected\\n\")\n",
        "!wc -l results/run_*.res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Package for Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è IMPORTANT: Use SUBMISSION files (199 test queries only) for competition!\n",
        "# NOT the full files (which include training queries)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create submission directory with correctly named files\n",
        "os.makedirs('submission', exist_ok=True)\n",
        "shutil.copy('results/run_1_submission.res', 'submission/run_1.res')\n",
        "shutil.copy('results/run_2_submission.res', 'submission/run_2.res')\n",
        "shutil.copy('results/run_3_submission.res', 'submission/run_3.res')\n",
        "\n",
        "# Verify files\n",
        "print(\"=== SUBMISSION FILES ===\")\n",
        "!ls -la submission/\n",
        "\n",
        "# Verify format (first few lines)\n",
        "print(\"\\n=== Sample lines from run_1.res ===\")\n",
        "!head -3 submission/run_1.res\n",
        "\n",
        "# Verify query IDs (should be 351-450 and 601-700, NOT 301-350)\n",
        "print(\"\\n=== First and last query IDs (should NOT include 301-350) ===\")\n",
        "!echo \"First 5 queries:\" && cut -d' ' -f1 submission/run_1.res | sort -u | head -5\n",
        "!echo \"Last 5 queries:\" && cut -d' ' -f1 submission/run_1.res | sort -u | tail -5\n",
        "\n",
        "# Create the final zip\n",
        "!cd submission && zip -r ../Final_Project_Part_A.zip run_1.res run_2.res run_3.res\n",
        "!ls -la Final_Project_Part_A.zip\n",
        "\n",
        "print(\"\\n‚úÖ Submission zip created successfully!\")\n",
        "print(\"üìã Contains: run_1.res, run_2.res, run_3.res (199 test queries each)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the submission file (for Colab)\n",
        "from google.colab import files\n",
        "files.download('Final_Project_Part_A.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary of Methods (Optimized for A100 80GB)\n",
        "\n",
        "| Run | Method | Key Techniques |\n",
        "|-----|--------|----------------|\n",
        "| 1 | BM25+RM3+Q2D | Claude Query2Doc expansions, tuned BM25/RM3, pseudo-relevance feedback |\n",
        "| 2 | Three-Stage Neural | **Bi-Encoder ‚Üí BGE-Reranker ‚Üí MonoT5-3B** pipeline, contextual chunking, MaxP |\n",
        "| 3 | RRF Fusion | Reciprocal Rank Fusion (k=60) of Run 1 and Run 2 |\n",
        "\n",
        "**Three-Stage Neural Pipeline (Run 2):**\n",
        "```\n",
        "BM25 (1000 docs) ‚Üí Bi-Encoder (top 500) ‚Üí Cross-Encoder + MonoT5 (ensemble) ‚Üí Final\n",
        "```\n",
        "\n",
        "**GPU Utilization (MAXED OUT for A100 80GB):**\n",
        "- **BGE-large-en-v1.5**: ~1.3GB VRAM, batch=**1024**\n",
        "- **BGE-Reranker-v2-m3**: ~2.3GB VRAM, batch=**1024**\n",
        "- **MonoT5-3B**: ~12GB VRAM, batch=**256**\n",
        "- **Total**: ~17GB VRAM + large batches = **~40-50GB peak usage**\n",
        "- Rerank depth: 200 docs ‚Üí filter to 500 ‚Üí final ensemble\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
