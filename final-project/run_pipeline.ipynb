{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ROBUST04 Ranking Competition - Ultimate Best Practices Pipeline\n",
        "\n",
        "This notebook runs the complete pipeline for the ROBUST04 ranking competition.\n",
        "\n",
        "**Three Methods:**\n",
        "1. **Run 1**: BM25 + RM3 + Claude Query2Doc (lexical + PRF + semantic expansion)\n",
        "2. **Run 2**: Neural MaxP Reranking (Cross-Encoder on passages)\n",
        "3. **Run 3**: Optimal Multi-Signal RRF Fusion\n",
        "\n",
        "**Expected Performance:**\n",
        "- Run 1 (Lexical): MAP ~0.33-0.36\n",
        "- Run 2 (Neural): MAP ~0.37-0.40\n",
        "- Run 3 (Fusion): MAP ~0.40-0.44\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository (replace with your repo URL)\n",
        "!git clone https://github.com/YOUR_USERNAME/text_retrieval.git\n",
        "%cd text_retrieval/final-project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Java (required by pyserini/Lucene)\n",
        "!apt-get update -qq\n",
        "!apt-get install -qq openjdk-21-jdk-headless > /dev/null\n",
        "\n",
        "# Verify Java installation\n",
        "!java -version\n",
        "\n",
        "# Install Python dependencies\n",
        "%pip install -q pyserini faiss-cpu torch transformers sentence-transformers \\\n",
        "    pytrec_eval langchain langchain-text-splitters tqdm scikit-learn numpy accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Pyserini's prebuilt ROBUST04 index\n",
        "print(\"Downloading ROBUST04 index (this may take a few minutes on first run)...\")\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "searcher = LuceneSearcher.from_prebuilt_index('robust04')\n",
        "print(f\"✓ Index loaded successfully!\")\n",
        "print(f\"  Total documents: {searcher.num_docs:,}\")\n",
        "searcher.close()\n",
        "\n",
        "# Pre-download neural models to cache them\n",
        "print(\"\\nPre-downloading neural models...\")\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Download Cross-Encoder\n",
        "print(\"  Downloading Cross-Encoder (ms-marco-MiniLM-L-12-v2)...\")\n",
        "ce_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device='cpu')\n",
        "del ce_model\n",
        "print(\"  ✓ Cross-Encoder downloaded\")\n",
        "\n",
        "# Download MonoT5 tokenizer (model downloaded on-demand if used)\n",
        "print(\"  Downloading T5 tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('castorini/monot5-base-msmarco')\n",
        "del tokenizer\n",
        "print(\"  ✓ T5 tokenizer downloaded\")\n",
        "\n",
        "print(\"\\n✓ All models and indexes ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation: Test All Components\n",
        "\n",
        "Run this cell to verify all libraries are working correctly before running the full pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate all components work correctly\n",
        "print(\"=\" * 50)\n",
        "print(\"VALIDATION: Testing all components\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test 1: BM25 Search\n",
        "print(\"\\n1. Testing BM25 search...\")\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "searcher = LuceneSearcher.from_prebuilt_index('robust04')\n",
        "searcher.set_bm25(k1=0.9, b=0.4)\n",
        "hits = searcher.search('international organized crime', k=10)\n",
        "print(f\"   ✓ BM25 returned {len(hits)} results\")\n",
        "print(f\"   Top doc: {hits[0].docid} (score: {hits[0].score:.4f})\")\n",
        "\n",
        "# Test 2: BM25 + RM3 Search\n",
        "print(\"\\n2. Testing BM25 + RM3 search...\")\n",
        "searcher.set_rm3(fb_docs=10, fb_terms=10, original_query_weight=0.5)\n",
        "hits_rm3 = searcher.search('international organized crime', k=10)\n",
        "print(f\"   ✓ BM25+RM3 returned {len(hits_rm3)} results\")\n",
        "searcher.close()\n",
        "\n",
        "# Test 3: Cross-Encoder scoring\n",
        "print(\"\\n3. Testing Cross-Encoder...\")\n",
        "from sentence_transformers import CrossEncoder\n",
        "ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device='cuda')\n",
        "scores = ce.predict([\n",
        "    ('international crime', 'This document discusses transnational criminal organizations.'),\n",
        "    ('international crime', 'The weather today is sunny and warm.')\n",
        "])\n",
        "print(f\"   ✓ Relevant doc score: {scores[0]:.4f}\")\n",
        "print(f\"   ✓ Irrelevant doc score: {scores[1]:.4f}\")\n",
        "del ce\n",
        "\n",
        "# Test 4: LangChain chunking\n",
        "print(\"\\n4. Testing LangChain chunking...\")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=256, chunk_overlap=64,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \", \", \" \", \"\"]\n",
        ")\n",
        "chunks = splitter.split_text(\"This is a test document. \" * 50)\n",
        "print(f\"   ✓ Created {len(chunks)} chunks from test text\")\n",
        "\n",
        "# Test 5: pytrec_eval\n",
        "print(\"\\n5. Testing pytrec_eval...\")\n",
        "import pytrec_eval\n",
        "qrels = {'q1': {'d1': 1, 'd2': 0, 'd3': 1}}\n",
        "results = {'q1': {'d1': 0.9, 'd2': 0.5, 'd3': 0.8}}\n",
        "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'ndcg'})\n",
        "metrics = evaluator.evaluate(results)\n",
        "print(f\"   ✓ MAP: {metrics['q1']['map']:.4f}\")\n",
        "print(f\"   ✓ NDCG: {metrics['q1']['ndcg']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"✓ All components validated successfully!\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Parameter Tuning (Optional - Run Once)\n",
        "\n",
        "This tunes BM25 and RM3 parameters using 5-fold cross-validation on the 50 training queries.\n",
        "\n",
        "**Takes ~15-30 minutes.** Results are saved so you only need to run this once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run parameter tuning (optional - skip if using defaults)\n",
        "!python -m src.main tune --output tuning_results/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Generate Run 1 - BM25 + RM3 + Query2Doc\n",
        "\n",
        "Uses Claude's pre-generated query expansions + tuned BM25/RM3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main run1 \\\n",
        "    --config tuning_results/best_config.json \\\n",
        "    --output results/run_1.res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Generate Run 2 - Neural MaxP Reranking\n",
        "\n",
        "Uses Cross-Encoder to rerank passages, then aggregates to document scores using MaxP.\n",
        "\n",
        "**Takes ~30-60 minutes** depending on GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main run2 \\\n",
        "    --config tuning_results/best_config.json \\\n",
        "    --output results/run_2.res \\\n",
        "    --rerank-depth 100 \\\n",
        "    --gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Generate Run 3 - Optimal RRF Fusion\n",
        "\n",
        "Fuses Run 1 and Run 2 using Reciprocal Rank Fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main run3 \\\n",
        "    --run1 results/run_1.res \\\n",
        "    --run2 results/run_2.res \\\n",
        "    --output results/run_3.res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Evaluate on Training Queries\n",
        "\n",
        "Check performance on the 50 training queries (301-350).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m src.main evaluate \\\n",
        "    results/run_1.res \\\n",
        "    results/run_2.res \\\n",
        "    results/run_3.res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Verify Output Format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check file sizes and line counts\n",
        "!echo \"=== Run 1 ===\"\n",
        "!wc -l results/run_1.res\n",
        "!head -5 results/run_1.res\n",
        "!echo \"\"\n",
        "!echo \"=== Run 2 ===\"\n",
        "!wc -l results/run_2.res\n",
        "!head -5 results/run_2.res\n",
        "!echo \"\"\n",
        "!echo \"=== Run 3 ===\"\n",
        "!wc -l results/run_3.res\n",
        "!head -5 results/run_3.res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify query coverage (should have 199 test queries)\n",
        "!echo \"Unique queries in run_1:\"\n",
        "!cut -d' ' -f1 results/run_1.res | sort -u | wc -l\n",
        "!echo \"Unique queries in run_2:\"\n",
        "!cut -d' ' -f1 results/run_2.res | sort -u | wc -l\n",
        "!echo \"Unique queries in run_3:\"\n",
        "!cut -d' ' -f1 results/run_3.res | sort -u | wc -l\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Package for Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission zip\n",
        "!zip -r Final_Project_Part_A.zip results/run_1.res results/run_2.res results/run_3.res\n",
        "!ls -la Final_Project_Part_A.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the submission file (for Colab)\n",
        "from google.colab import files\n",
        "files.download('Final_Project_Part_A.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary of Methods\n",
        "\n",
        "| Run | Method | Key Techniques |\n",
        "|-----|--------|----------------|\n",
        "| 1 | BM25+RM3+Q2D | Claude Query2Doc expansions, tuned BM25/RM3, pseudo-relevance feedback |\n",
        "| 2 | Neural MaxP | Cross-Encoder (L-12), contextual chunking, MaxP aggregation |\n",
        "| 3 | RRF Fusion | Reciprocal Rank Fusion (k=60) of Run 1 and Run 2 |\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
